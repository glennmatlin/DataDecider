# configs/training_configs/test_4m_training.yaml
# Minimal training configuration for testing 4M model
training:
  # Model selection
  model_size: "4M"
  
  # Training hyperparameters - minimal for testing
  learning_rate: 1e-3
  batch_size: 4
  gradient_accumulation_steps: 1
  num_train_epochs: 1
  max_steps: 100  # Just 100 steps for testing
  warmup_steps: 10
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Optimization
  optimizer: "adamw"
  lr_scheduler_type: "constant"  # Simple for testing
  
  # Mixed precision
  fp16: false  # Disable for testing
  bf16: false
  
  # Checkpointing
  save_strategy: "steps"
  save_steps: 50  # Save at 50 and 100 steps
  save_total_limit: 2
  load_best_model_at_end: false
  
  # Logging
  logging_steps: 10
  logging_first_step: true
  report_to: []  # Disable external logging for test
  
  # Evaluation
  evaluation_strategy: "steps"
  eval_steps: 50
  per_device_eval_batch_size: 4
  
  # Distributed training
  ddp_find_unused_parameters: false
  gradient_checkpointing: false  # Disable for small model
  
  # Data
  max_length: 512  # Shorter sequences for testing
  num_workers: 2
  
  # Random seed
  seed: 42