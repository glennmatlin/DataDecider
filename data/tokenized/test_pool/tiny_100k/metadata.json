{
  "tokenization_config": {
    "input_path": "data/raw/arxiv_sample.json.gz",
    "output_path": "data/tokenized/test_pool/tiny_100k",
    "tokenizer_name": "EleutherAI/gpt-neox-20b",
    "max_seq_length": 2048,
    "batch_size": 1000,
    "validation_split": 0.1,
    "max_tokens": 100000,
    "append_eos": true,
    "save_format": "arrow",
    "num_proc": 4,
    "resume": true
  },
  "tokenizer_info": {
    "name": "EleutherAI/gpt-neox-20b",
    "vocab_size": 50254,
    "eos_token": "<|endoftext|>",
    "eos_token_id": 0,
    "pad_token": "<|endoftext|>"
  },
  "statistics": {
    "total_documents": 8,
    "total_tokens": 108815,
    "total_sequences": 42,
    "avg_doc_length": 13601.875,
    "max_doc_length": 29225,
    "min_doc_length": 4101
  },
  "dataset_info": {
    "train_samples": 37,
    "validation_samples": 5,
    "sequence_length": 2048
  },
  "created_at": "2025-06-25T17:40:31.676897",
  "datadecider_version": "0.1.0",
  "checksums": {
    "train/data-00000-of-00001.arrow": "dee7ee6ab614b149d700da15b3d5a21bd1967c5db5ca6634fcd02ae7cce4d472",
    "validation/data-00000-of-00001.arrow": "eea2d29745920ca42d16272b4d3a1b133fee667ee67ac4002c2c95a8fd8e8a19"
  },
  "description": "Tiny dataset for unit tests and debugging",
  "pool_type": "test"
}
