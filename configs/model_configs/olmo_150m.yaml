# configs/model_configs/olmo_150m.yaml
model:
  name: "olmo-150m"
  vocab_size: 50257
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_act: "swiglu"
  hidden_dropout_prob: 0.0
  attention_probs_dropout_prob: 0.0
  max_position_embeddings: 2048
  initializer_range: 0.02
  layer_norm_eps: 1e-12
  use_cache: true
  rope_theta: 10000.0
  use_bias: false
  tie_word_embeddings: true

training_params:
  batch_size: 192
  learning_rate: 4.2e-03
  training_steps: 38157
  tokens_trained: 15.0  # Billion
