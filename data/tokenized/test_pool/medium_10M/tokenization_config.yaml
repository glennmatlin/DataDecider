append_eos: true
batch_size: 1000
input_path: data/raw/arxiv_sample.json.gz
max_seq_length: 2048
max_tokens: 10000000
num_proc: 4
output_path: data/tokenized/test_pool/medium_10M
resume: true
save_format: arrow
tokenizer_name: EleutherAI/gpt-neox-20b
validation_split: 0.1
