{
  "tokenization_config": {
    "input_path": "data/raw/arxiv_sample.json.gz",
    "output_path": "data/tokenized/test_pool/small_1M",
    "tokenizer_name": "EleutherAI/gpt-neox-20b",
    "max_seq_length": 2048,
    "batch_size": 1000,
    "validation_split": 0.1,
    "max_tokens": 1000000,
    "append_eos": true,
    "save_format": "arrow",
    "num_proc": 4,
    "resume": true
  },
  "tokenizer_info": {
    "name": "EleutherAI/gpt-neox-20b",
    "vocab_size": 50254,
    "eos_token": "<|endoftext|>",
    "eos_token_id": 0,
    "pad_token": "<|endoftext|>"
  },
  "statistics": {
    "total_documents": 51,
    "total_tokens": 1003418,
    "total_sequences": 488,
    "avg_doc_length": 19674.86274509804,
    "max_doc_length": 68765,
    "min_doc_length": 3175
  },
  "dataset_info": {
    "train_samples": 439,
    "validation_samples": 49,
    "sequence_length": 2048
  },
  "created_at": "2025-06-25T17:40:36.964321",
  "datadecider_version": "0.1.0",
  "checksums": {
    "train/data-00000-of-00001.arrow": "6aaa66eb1f360fa805f9e8f8fbdc656b78b41bf5aa4fdd7d21b1d06f09c3a37e",
    "validation/data-00000-of-00001.arrow": "86b4f92d277428962c4b1a0fe8aa892a4525e81a17d3ea3d041873cdbde90b1f"
  },
  "description": "Small dataset for quick experiments",
  "pool_type": "test"
}
